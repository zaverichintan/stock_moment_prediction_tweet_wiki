{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 87/87 [00:15<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHTR', 'MDT', 'RDS-B', 'BRK-A', 'AGFS', 'BABA']\n",
      "['unknown', 'unknown', 'unknown', 'unknown', 'unknown']\n",
      "#tickers selected: (87, 2)\n",
      "#paths selected: 114\n",
      "#connection items: 1108\n",
      "#tickers aligned: 68\n",
      "#valid paths: 57\n",
      "connections count: 732 ratio: 0.09671026555687673\n",
      "(87, 87, 58)\n",
      "376 128 61\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import datetime\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(\"runs/mansf-stocknet\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataloader import MixDataset\n",
    "from model import MANSF\n",
    "from utils import build_wiki_relation, accuracy\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.fastmode = False\n",
    "        self.sparse = False\n",
    "        self.seed = 14\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 8\n",
    "        self.lr = 5e-4\n",
    "        self.weight_decay = 5e-4\n",
    "        self.hidden = 64\n",
    "        self.nb_heads = 8\n",
    "        self.dropout = 0.38\n",
    "        self.alpha = 0.2\n",
    "        self.patience = 100\n",
    "        self.window = 5\n",
    "        self.max_tweet_num = 5\n",
    "        self.max_tweet_len = 30\n",
    "        self.text_ft_dim = 384\n",
    "\n",
    "args = Args()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "\n",
    "def gen_dataset(dataframe_dict: dict, start_date: str, end_date: str):\n",
    "    \"\"\"generate data with different date range\"\"\"\n",
    "    for name in dataframe_dict.keys():\n",
    "#         dataframe_dict[name] = (\n",
    "#             dataframe_dict[name].drop_duplicates().reset_index(drop=True)\n",
    "#         )\n",
    "        dataframe_dict[name] = dataframe_dict[name][\n",
    "            (dataframe_dict[name][\"date\"].notnull())\n",
    "            & (dataframe_dict[name][\"date\"] >= start_date)\n",
    "            & (dataframe_dict[name][\"date\"] <= end_date)\n",
    "        ].reset_index(drop=True)\n",
    "    prices, tweets = dataframe_dict[\"price\"], dataframe_dict[\"tweet\"]\n",
    "    prices = prices.sort_values(\"date\").reset_index(drop=True)\n",
    "    tweets = (\n",
    "        tweets.groupby([\"stock\", \"date\"], as_index=False)\n",
    "        .agg({\"text\": \"\\n\".join})\n",
    "        .fillna(\"\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    mix = (\n",
    "        pd.merge(prices, tweets, on=[\"stock\", \"date\"], how=\"left\")\n",
    "        .fillna(\"\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    mix_pv = pd.pivot(mix, index=\"date\", columns=\"stock\").reset_index()\n",
    "    mix_pv[\"text\"] = mix_pv[\"text\"].fillna(\"\")\n",
    "    for col in [\"movement_perc\", \"high\", \"low\", \"close\"]:\n",
    "        mix_pv[col] = mix_pv[col].fillna(0.0)\n",
    "    return mix_pv\n",
    "\n",
    "\n",
    "stock_dir = \"stocknet-dataset/price/preprocessed\"\n",
    "tweet_dir = \"stocknet-dataset/tweet_preprocessed\"\n",
    "\n",
    "\n",
    "rels_dir = \"Temporal_Relational_Stock_Ranking/data/relation/wikidata\"\n",
    "market_names = [\"NASDAQ\", \"NYSE\"]\n",
    "\n",
    "args.stock_list = sorted(\n",
    "    list(\n",
    "        set(\n",
    "            [path.split(\"/\")[-1].replace(\".txt\", \"\") for path in glob(stock_dir + \"/*\")]\n",
    "        )\n",
    "        & set([path.split(\"/\")[-1].split('.')[0] for path in glob(tweet_dir + \"/*\")])\n",
    "    )\n",
    ")\n",
    "args.n_stock = len(args.stock_list)  # the number of stocks\n",
    "n_day = 5  # the backward-looking window T\n",
    "n_tweet = 5  # max num of tweets per day, I suppose 1 tweet per stock per day\n",
    "n_price_feat = 3  # price feature dim  (normalized high/low/close)\n",
    "n_tweet_feat = 384  # text embedding dim\n",
    "\n",
    "prices, tweets = pd.DataFrame(), pd.DataFrame()\n",
    "for stock in tqdm(args.stock_list):\n",
    "    _p = pd.read_table(os.path.join(stock_dir, f\"{stock}.txt\"), header=None)\n",
    "    _t = pd.read_json(os.path.join(tweet_dir, f\"{stock}.json\"), orient=\"records\", lines=True)\n",
    "    \n",
    "    _t['text'] = [' '.join(map(str, l)) for l in _t['text']]\n",
    "    \n",
    "    _p[\"stock\"], _t[\"stock\"] = stock, stock\n",
    "    prices, tweets = pd.concat([prices, _p]), pd.concat([tweets, _t])\n",
    "\n",
    "prices.columns = [\n",
    "    \"date\",\n",
    "    \"movement_perc\",\n",
    "    \"open\",\n",
    "    \"high\",\n",
    "    \"low\",\n",
    "    \"close\",\n",
    "    \"volume\",\n",
    "    \"stock\",\n",
    "]\n",
    "prices = prices.drop([\"open\", \"volume\"], axis=1)\n",
    "tweets[\"date\"] = tweets.created_at.apply(lambda x: x.date())\n",
    "tweets.date = tweets.date.astype(str)\n",
    "\n",
    "mix_pv_train = gen_dataset(\n",
    "    dataframe_dict={\"price\": prices, \"tweet\": tweets},\n",
    "    start_date=\"2014-01-01\",\n",
    "    end_date=\"2015-06-30\",\n",
    ")\n",
    "mix_pv_val = gen_dataset(\n",
    "    dataframe_dict={\"price\": prices, \"tweet\": tweets},\n",
    "    start_date=\"2015-07-01\",\n",
    "    end_date=\"2015-12-31\",\n",
    ")\n",
    "mix_pv_test = gen_dataset(\n",
    "    dataframe_dict={\"price\": prices, \"tweet\": tweets},\n",
    "    start_date=\"2016-01-01\",\n",
    "    end_date=\"2016-03-31\",\n",
    ")\n",
    "\n",
    "# preprocess relation data & load\n",
    "adj = build_wiki_relation(rels_dir, market_names, args.stock_list)\n",
    "edge_index = torch.index_select(torch.nonzero(adj).t(), 0, torch.tensor([0, 1]))\n",
    "edge_type = torch.index_select(torch.nonzero(adj).t(), 0, torch.tensor([2])).squeeze(0)\n",
    "\n",
    "model = MANSF(\n",
    "    nfeat=64,\n",
    "    nhid=args.hidden,\n",
    "    nrel=adj.shape[2],\n",
    "    nclass=2,\n",
    "    dropout=args.dropout,\n",
    "    nheads=args.nb_heads,\n",
    "    alpha=args.alpha,\n",
    "    stock_num=args.n_stock,\n",
    "    text_ft_dim=args.text_ft_dim,\n",
    ")\n",
    "# model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    edge_index = edge_index.type(torch.LongTensor).cuda()\n",
    "    edge_type = edge_type.type(torch.LongTensor).cuda()\n",
    "    # adj = adj.type(torch.LongTensor).cuda()\n",
    "    args.device = \"cuda\"\n",
    "\n",
    "trainset = MixDataset(\n",
    "    mode=\"train\",\n",
    "    data=mix_pv_train,\n",
    "    window_num=args.window,\n",
    "    max_tweet_num=args.max_tweet_num,\n",
    "    max_tweet_len=args.max_tweet_len,\n",
    "    stock_list=args.stock_list,\n",
    ")\n",
    "valset = MixDataset(\n",
    "    mode=\"val\",\n",
    "    data=mix_pv_val,\n",
    "    window_num=args.window,\n",
    "    max_tweet_num=args.max_tweet_num,\n",
    "    max_tweet_len=args.max_tweet_len,\n",
    "    stock_list=args.stock_list,\n",
    ")\n",
    "testset = MixDataset(\n",
    "    mode=\"test\",\n",
    "    data=mix_pv_test,\n",
    "    window_num=args.window,\n",
    "    max_tweet_num=args.max_tweet_num,\n",
    "    max_tweet_len=args.max_tweet_len,\n",
    "    stock_list=args.stock_list,\n",
    ")\n",
    "trainsampler = RandomSampler(trainset)\n",
    "valsampler = RandomSampler(valset)\n",
    "testsampler = RandomSampler(testset)\n",
    "trainloader = DataLoader(\n",
    "    trainset, sampler=trainsampler, batch_size=args.batch_size, drop_last=True\n",
    ")\n",
    "valloader = DataLoader(\n",
    "    valset, sampler=valsampler, batch_size=args.batch_size, drop_last=True\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, sampler=testsampler, batch_size=args.batch_size, drop_last=True\n",
    ")\n",
    "print(len(trainset), len(valset), len(testset))\n",
    "\n",
    "# train(args, model, trainloader, valloader, edge_index, edge_type)\n",
    "# print(\"Optimization Finished!\")\n",
    "# results = test_dict()\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 22.24it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 22.21it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 23.01it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 22.53it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 23.33it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 22.55it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 22.00it/s]\n",
      "Collect from all stocks: 100%|██████████| 86/86 [00:03<00:00, 22.25it/s]\n",
      "Training:   0%|          | 0/47 [00:30<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    ")\n",
    "loss_fn = nn.BCELoss().cuda()\n",
    "for i, data in enumerate(tqdm(trainloader, desc=\"Training\")):\n",
    "    text_input, price_input, label_input = data\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeat=64\n",
    "nhid=args.hidden\n",
    "nrel=adj.shape[2]\n",
    "nclass=2\n",
    "dropout=args.dropout\n",
    "nheads=args.nb_heads\n",
    "alpha=args.alpha\n",
    "stock_num=args.n_stock\n",
    "text_ft_dim=args.text_ft_dim\n",
    "# layer func\n",
    "from torch_geometric.nn import RGATConv\n",
    "class gru(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(gru, self).__init__()\n",
    "        self.gru1 = nn.GRU(\n",
    "            input_size=input_size, hidden_size=hidden_size, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        full, last = self.gru1(inputs)\n",
    "        return full, last\n",
    "\n",
    "\n",
    "class attn(nn.Module):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super(attn, self).__init__()\n",
    "        self.W1 = nn.Linear(in_shape, out_shape)\n",
    "        self.W2 = nn.Linear(in_shape, out_shape)\n",
    "        self.V = nn.Linear(in_shape, 1)\n",
    "\n",
    "    def forward(self, full, last):\n",
    "        score = self.V(torch.tanh(self.W1(last) + self.W2(full)))\n",
    "        attention_weights = F.softmax(score, dim=1)\n",
    "        context_vector = attention_weights * full\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "# layers \n",
    "grup = [gru(3, nhid).cuda() for _ in range(stock_num)]\n",
    "attnp = [attn(nhid, nhid).cuda() for _ in range(stock_num)]\n",
    "tweet_gru = [gru(text_ft_dim, nhid).cuda() for _ in range(stock_num)]\n",
    "grut = [gru(nhid, nhid).cuda() for _ in range(stock_num)]\n",
    "attn_tweet = [attn(nhid, nhid).cuda() for _ in range(stock_num)]\n",
    "attnt = [attn(nhid, nhid).cuda() for _ in range(stock_num)]\n",
    "bilinear = [nn.Bilinear(nhid, nhid, nhid).cuda() for _ in range(stock_num)]\n",
    "linear_x = nn.Linear(nhid, 2).cuda()\n",
    "dropout_ratio = dropout\n",
    "attentions = RGATConv(nfeat, nhid, nrel, dropout=dropout_ratio, alpha=alpha, concat=True).cuda()\n",
    "out_att = RGATConv(nhid, nclass, nrel, dropout=dropout_ratio, alpha=alpha, concat=False).cuda()\n",
    "out_classify = nn.Softmax(dim=1).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "batch_size = text_input.size(0)\n",
    "num_tw, tw_ft_dim = text_input.size(3), text_input.size(4)\n",
    "num_d = price_input.size(2)\n",
    "pr_ft = price_input.size(3)\n",
    "num_stocks = price_input.size(1)\n",
    "\n",
    "for i in range(num_stocks):  # n_stock\n",
    "    # price data\n",
    "    x = grup[i](\n",
    "        price_input[:, i, :, :].reshape((batch_size, num_d, pr_ft)).float()\n",
    "    )  # [[(b, 5, 64), (1, b, 64)], [(b, 5, 64), (1, b, 64)], ,,,]\n",
    "    x = (x[0], x[1].reshape((batch_size, 1, nhid)))\n",
    "    x = attnp[i](*x).reshape((batch_size, nhid))  # (b, 64)\n",
    "    # x = layer_normp[i](x).reshape(batch_size, 64)\n",
    "    han_li1 = []\n",
    "    for j in range(num_d):  # n_day\n",
    "        # tweet of each day\n",
    "        y = tweet_gru[i](\n",
    "            text_input[:, i, j, :, :].reshape((batch_size, num_tw, tw_ft_dim))\n",
    "        )  # [(b, num_tw, emb_dim), (1, b, emb_dim),...]\n",
    "        y = (y[0], y[1].reshape((batch_size, 1, nhid)))\n",
    "        y = attn_tweet[i](*y).reshape((batch_size, nhid))  # (b, 64)\n",
    "        han_li1.append(y)\n",
    "    # tweets in window days\n",
    "    # news_vector = torch.Tensor((batch_size, num_d, 64))\n",
    "    news_vector = torch.cat(han_li1)  # (b * 5, 64)\n",
    "    text = grut[i](\n",
    "        news_vector.reshape(batch_size, num_d, nhid)\n",
    "    )  # [(b, 5, 64), (1, b, 64),...]\n",
    "    text = (text[0], text[1].reshape((batch_size, 1, nhid)))\n",
    "    text = attnt[i](*text).reshape((batch_size, nhid))  # (b, 64)\n",
    "    # tweet X price\n",
    "    combined = torch.tanh(\n",
    "        bilinear[i](text, x).reshape((batch_size, nhid))\n",
    "    )  # (b, 64)\n",
    "    li.append(combined.reshape(batch_size, nhid))\n",
    "\n",
    "ft_vec = torch.cat(li).reshape((batch_size, num_stocks, nhid))  # (b, n_stock, 64)\n",
    "out_1 = torch.tanh(linear_x(ft_vec))  # (b, n_stock, 2)\n",
    "x = F.dropout(ft_vec, dropout_ratio)  # (b, n_stock, 64)\n",
    "\n",
    "x, att1 = attentions(x[0], edge_index, edge_type, return_attention_weights=True)  # (n_stock, 64)\n",
    "x = F.dropout(x, dropout_ratio)  # (n_stock, 64)\n",
    "x, att2 = out_att(x, edge_index, edge_type, return_attention_weights=True)\n",
    "x = F.elu(x)  # (n_stock, 2)\n",
    "res = out_classify(x + out_1)  # (n_stock, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0974],\n",
       "        [0.0861],\n",
       "        [0.0876],\n",
       "        [0.0814],\n",
       "        [0.0809],\n",
       "        [0.0639],\n",
       "        [0.0629],\n",
       "        [0.0560],\n",
       "        [0.0609],\n",
       "        [0.0549],\n",
       "        [1.0000],\n",
       "        [0.0461],\n",
       "        [0.0449],\n",
       "        [0.0388],\n",
       "        [0.0544],\n",
       "        [0.1344],\n",
       "        [0.0551],\n",
       "        [0.0656],\n",
       "        [0.1377],\n",
       "        [0.1391],\n",
       "        [0.0372],\n",
       "        [0.0329],\n",
       "        [0.0433],\n",
       "        [0.0454],\n",
       "        [0.0403],\n",
       "        [0.0462],\n",
       "        [0.1174],\n",
       "        [0.0514],\n",
       "        [0.0661],\n",
       "        [0.0299],\n",
       "        [0.0331],\n",
       "        [0.0551],\n",
       "        [1.0000],\n",
       "        [0.1259],\n",
       "        [0.0816],\n",
       "        [0.1120],\n",
       "        [0.1105],\n",
       "        [0.0828],\n",
       "        [0.1134],\n",
       "        [0.0620],\n",
       "        [0.0545],\n",
       "        [0.1230],\n",
       "        [0.0729],\n",
       "        [0.1087],\n",
       "        [0.0783],\n",
       "        [0.0860],\n",
       "        [0.1075],\n",
       "        [0.0834],\n",
       "        [0.0656],\n",
       "        [0.0610],\n",
       "        [0.0619],\n",
       "        [0.0582],\n",
       "        [0.0536],\n",
       "        [0.0547],\n",
       "        [0.2636],\n",
       "        [0.0674],\n",
       "        [0.0608],\n",
       "        [0.0600],\n",
       "        [1.0000],\n",
       "        [0.0471],\n",
       "        [0.0449],\n",
       "        [0.0474],\n",
       "        [0.0462],\n",
       "        [0.0391],\n",
       "        [0.0546],\n",
       "        [0.0665],\n",
       "        [0.0432],\n",
       "        [0.0426],\n",
       "        [0.0555],\n",
       "        [0.1654],\n",
       "        [0.0661],\n",
       "        [0.0375],\n",
       "        [0.0332],\n",
       "        [0.0436],\n",
       "        [0.0463],\n",
       "        [0.0515],\n",
       "        [0.0662],\n",
       "        [0.1649],\n",
       "        [0.0300],\n",
       "        [0.0333],\n",
       "        [0.0556],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.1424],\n",
       "        [0.1232],\n",
       "        [0.1370],\n",
       "        [0.0480],\n",
       "        [0.1395],\n",
       "        [0.0621],\n",
       "        [0.0460],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0441],\n",
       "        [0.0445],\n",
       "        [0.0481],\n",
       "        [0.0701],\n",
       "        [0.0367],\n",
       "        [0.0600],\n",
       "        [0.0653],\n",
       "        [0.0665],\n",
       "        [0.0423],\n",
       "        [0.0430],\n",
       "        [0.0426],\n",
       "        [0.0755],\n",
       "        [0.1645],\n",
       "        [0.0357],\n",
       "        [0.0305],\n",
       "        [0.0381],\n",
       "        [0.0604],\n",
       "        [0.1640],\n",
       "        [0.0294],\n",
       "        [0.0325],\n",
       "        [0.0436],\n",
       "        [0.4805],\n",
       "        [0.5340],\n",
       "        [0.1238],\n",
       "        [0.1282],\n",
       "        [0.0780],\n",
       "        [0.1087],\n",
       "        [0.0792],\n",
       "        [0.1084],\n",
       "        [0.0817],\n",
       "        [0.0614],\n",
       "        [0.0540],\n",
       "        [0.3468],\n",
       "        [0.0549],\n",
       "        [0.0343],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0946],\n",
       "        [0.1245],\n",
       "        [0.0802],\n",
       "        [0.0732],\n",
       "        [0.1101],\n",
       "        [0.0809],\n",
       "        [0.1115],\n",
       "        [0.0826],\n",
       "        [0.0617],\n",
       "        [0.0618],\n",
       "        [0.0543],\n",
       "        [0.0546],\n",
       "        [0.1499],\n",
       "        [0.1310],\n",
       "        [0.1498],\n",
       "        [0.0526],\n",
       "        [0.1490],\n",
       "        [0.0679],\n",
       "        [0.0503],\n",
       "        [0.0439],\n",
       "        [1.0000],\n",
       "        [0.0461],\n",
       "        [0.0723],\n",
       "        [0.0368],\n",
       "        [0.0602],\n",
       "        [0.0424],\n",
       "        [0.0757],\n",
       "        [0.0358],\n",
       "        [0.0306],\n",
       "        [0.0315],\n",
       "        [0.0382],\n",
       "        [0.0380],\n",
       "        [0.0295],\n",
       "        [0.0326],\n",
       "        [0.0437],\n",
       "        [0.0458],\n",
       "        [0.0441],\n",
       "        [0.0485],\n",
       "        [0.0714],\n",
       "        [0.0402],\n",
       "        [0.0645],\n",
       "        [0.0542],\n",
       "        [0.0443],\n",
       "        [0.0773],\n",
       "        [0.0541],\n",
       "        [0.0644],\n",
       "        [0.0364],\n",
       "        [0.0366],\n",
       "        [0.0311],\n",
       "        [0.0323],\n",
       "        [0.0431],\n",
       "        [0.0396],\n",
       "        [0.0461],\n",
       "        [0.0512],\n",
       "        [0.0659],\n",
       "        [0.0300],\n",
       "        [0.0298],\n",
       "        [0.0331],\n",
       "        [0.0329],\n",
       "        [0.0541],\n",
       "        [0.0446],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.1228],\n",
       "        [0.0746],\n",
       "        [0.1084],\n",
       "        [0.0782],\n",
       "        [0.1071],\n",
       "        [0.0803],\n",
       "        [0.0826],\n",
       "        [0.0609],\n",
       "        [0.0535],\n",
       "        [0.2465],\n",
       "        [0.0459],\n",
       "        [0.0701],\n",
       "        [0.0367],\n",
       "        [0.0666],\n",
       "        [0.0548],\n",
       "        [0.0422],\n",
       "        [0.0755],\n",
       "        [0.0540],\n",
       "        [0.1865],\n",
       "        [0.0357],\n",
       "        [0.0305],\n",
       "        [0.0381],\n",
       "        [0.0294],\n",
       "        [0.0325],\n",
       "        [0.0436],\n",
       "        [0.5195],\n",
       "        [0.4660],\n",
       "        [0.1076],\n",
       "        [0.1064],\n",
       "        [0.1113],\n",
       "        [0.0733],\n",
       "        [0.1096],\n",
       "        [0.0854],\n",
       "        [0.1069],\n",
       "        [0.1102],\n",
       "        [0.0927],\n",
       "        [0.0618],\n",
       "        [0.0618],\n",
       "        [0.0546],\n",
       "        [1.0000],\n",
       "        [0.0469],\n",
       "        [0.2459],\n",
       "        [0.0452],\n",
       "        [0.0390],\n",
       "        [0.0602],\n",
       "        [0.0550],\n",
       "        [0.0554],\n",
       "        [0.0534],\n",
       "        [0.0659],\n",
       "        [0.0374],\n",
       "        [0.0331],\n",
       "        [0.0435],\n",
       "        [0.0463],\n",
       "        [0.0515],\n",
       "        [0.0662],\n",
       "        [0.0299],\n",
       "        [0.0332],\n",
       "        [0.0554],\n",
       "        [1.0000],\n",
       "        [0.0469],\n",
       "        [0.0496],\n",
       "        [0.0502],\n",
       "        [0.0678],\n",
       "        [0.0453],\n",
       "        [0.0457],\n",
       "        [0.0426],\n",
       "        [0.1750],\n",
       "        [0.0313],\n",
       "        [0.0405],\n",
       "        [0.0628],\n",
       "        [0.1745],\n",
       "        [0.0307],\n",
       "        [0.0338],\n",
       "        [0.0328],\n",
       "        [0.0456],\n",
       "        [1.0000],\n",
       "        [0.0964],\n",
       "        [0.0967],\n",
       "        [0.1251],\n",
       "        [0.0759],\n",
       "        [0.0821],\n",
       "        [0.0732],\n",
       "        [0.1128],\n",
       "        [0.0834],\n",
       "        [0.0850],\n",
       "        [0.1142],\n",
       "        [0.0824],\n",
       "        [0.0643],\n",
       "        [0.0555],\n",
       "        [0.0545],\n",
       "        [0.0546],\n",
       "        [0.0546],\n",
       "        [0.0475],\n",
       "        [0.1404],\n",
       "        [0.1427],\n",
       "        [0.1410],\n",
       "        [0.0485],\n",
       "        [0.0430],\n",
       "        [0.1247],\n",
       "        [0.0452],\n",
       "        [0.0447],\n",
       "        [0.0458],\n",
       "        [0.0483],\n",
       "        [0.0463],\n",
       "        [0.0699],\n",
       "        [0.0366],\n",
       "        [0.0598],\n",
       "        [0.0651],\n",
       "        [0.0671],\n",
       "        [0.0435],\n",
       "        [0.0753],\n",
       "        [0.1686],\n",
       "        [0.0356],\n",
       "        [0.0305],\n",
       "        [0.0380],\n",
       "        [0.0603],\n",
       "        [0.1680],\n",
       "        [0.0294],\n",
       "        [0.0324],\n",
       "        [0.0338],\n",
       "        [0.0434],\n",
       "        [0.0482],\n",
       "        [0.0713],\n",
       "        [0.0373],\n",
       "        [0.0640],\n",
       "        [0.0440],\n",
       "        [0.0840],\n",
       "        [0.0364],\n",
       "        [0.0311],\n",
       "        [0.0317],\n",
       "        [0.0393],\n",
       "        [0.0299],\n",
       "        [0.0330],\n",
       "        [0.0443],\n",
       "        [0.0490],\n",
       "        [0.2439],\n",
       "        [0.0472],\n",
       "        [0.0407],\n",
       "        [0.0597],\n",
       "        [0.0569],\n",
       "        [0.0540],\n",
       "        [0.0523],\n",
       "        [0.0689],\n",
       "        [0.0391],\n",
       "        [0.0346],\n",
       "        [0.0454],\n",
       "        [0.0474],\n",
       "        [0.0533],\n",
       "        [0.0672],\n",
       "        [0.0311],\n",
       "        [0.0347],\n",
       "        [0.0579],\n",
       "        [0.0458],\n",
       "        [0.0490],\n",
       "        [0.0681],\n",
       "        [0.0447],\n",
       "        [0.1623],\n",
       "        [0.1704],\n",
       "        [0.0480],\n",
       "        [0.0462],\n",
       "        [0.0399],\n",
       "        [0.0557],\n",
       "        [0.0567],\n",
       "        [0.0674],\n",
       "        [0.0383],\n",
       "        [0.0339],\n",
       "        [0.0445],\n",
       "        [0.0465],\n",
       "        [0.0522],\n",
       "        [0.0665],\n",
       "        [0.0305],\n",
       "        [0.0340],\n",
       "        [0.0567],\n",
       "        [1.0000],\n",
       "        [0.1890],\n",
       "        [0.0441],\n",
       "        [0.0375],\n",
       "        [0.0302],\n",
       "        [0.0562],\n",
       "        [0.0600],\n",
       "        [0.1862],\n",
       "        [0.0358],\n",
       "        [0.3311],\n",
       "        [0.0505],\n",
       "        [0.1512],\n",
       "        [0.1407],\n",
       "        [0.1489],\n",
       "        [0.0515],\n",
       "        [0.0457],\n",
       "        [0.1325],\n",
       "        [0.0961],\n",
       "        [0.0960],\n",
       "        [0.0975],\n",
       "        [0.1265],\n",
       "        [0.0747],\n",
       "        [0.0869],\n",
       "        [0.0735],\n",
       "        [0.1192],\n",
       "        [0.0882],\n",
       "        [0.0865],\n",
       "        [0.1207],\n",
       "        [0.0805],\n",
       "        [0.0839],\n",
       "        [0.0622],\n",
       "        [0.0640],\n",
       "        [0.0627],\n",
       "        [0.0620],\n",
       "        [0.0552],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0526],\n",
       "        [0.1575],\n",
       "        [0.1581],\n",
       "        [0.1507],\n",
       "        [0.0537],\n",
       "        [0.0476],\n",
       "        [0.1381],\n",
       "        [0.0511],\n",
       "        [0.0492],\n",
       "        [0.0467],\n",
       "        [0.0709],\n",
       "        [0.0371],\n",
       "        [0.0424],\n",
       "        [0.0621],\n",
       "        [0.0593],\n",
       "        [0.0427],\n",
       "        [0.0763],\n",
       "        [0.0603],\n",
       "        [0.0718],\n",
       "        [0.1886],\n",
       "        [0.0402],\n",
       "        [0.0309],\n",
       "        [0.0360],\n",
       "        [0.0474],\n",
       "        [0.0385],\n",
       "        [0.0494],\n",
       "        [0.0555],\n",
       "        [0.0700],\n",
       "        [0.0298],\n",
       "        [0.0324],\n",
       "        [0.0328],\n",
       "        [0.0361],\n",
       "        [0.0603],\n",
       "        [0.0441],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0461],\n",
       "        [0.0443],\n",
       "        [0.0482],\n",
       "        [0.0713],\n",
       "        [0.0720],\n",
       "        [0.0373],\n",
       "        [0.0383],\n",
       "        [0.0641],\n",
       "        [0.0542],\n",
       "        [0.0664],\n",
       "        [0.0440],\n",
       "        [0.0768],\n",
       "        [0.0774],\n",
       "        [0.0543],\n",
       "        [0.0647],\n",
       "        [0.0364],\n",
       "        [0.0367],\n",
       "        [0.0329],\n",
       "        [0.0432],\n",
       "        [0.0393],\n",
       "        [0.0382],\n",
       "        [0.0461],\n",
       "        [0.0513],\n",
       "        [0.0615],\n",
       "        [0.0659],\n",
       "        [0.0300],\n",
       "        [0.0298],\n",
       "        [0.0330],\n",
       "        [0.0330],\n",
       "        [0.0544],\n",
       "        [0.0443],\n",
       "        [0.0469],\n",
       "        [0.0465],\n",
       "        [0.0452],\n",
       "        [0.0390],\n",
       "        [0.0544],\n",
       "        [0.1392],\n",
       "        [0.0554],\n",
       "        [0.0659],\n",
       "        [0.1977],\n",
       "        [0.1396],\n",
       "        [0.1403],\n",
       "        [0.0374],\n",
       "        [0.0331],\n",
       "        [0.0451],\n",
       "        [0.0421],\n",
       "        [0.0463],\n",
       "        [0.1220],\n",
       "        [0.0515],\n",
       "        [0.0662],\n",
       "        [0.0299],\n",
       "        [0.0332],\n",
       "        [0.0554],\n",
       "        [0.0447],\n",
       "        [0.0481],\n",
       "        [0.0713],\n",
       "        [0.0763],\n",
       "        [0.0373],\n",
       "        [0.0639],\n",
       "        [0.0664],\n",
       "        [0.1335],\n",
       "        [0.0439],\n",
       "        [0.0768],\n",
       "        [0.2226],\n",
       "        [0.1368],\n",
       "        [0.1382],\n",
       "        [0.0363],\n",
       "        [0.0311],\n",
       "        [0.0339],\n",
       "        [0.0439],\n",
       "        [0.0391],\n",
       "        [0.1166],\n",
       "        [0.0615],\n",
       "        [0.3352],\n",
       "        [0.0299],\n",
       "        [0.0330],\n",
       "        [0.0528],\n",
       "        [0.0443],\n",
       "        [1.0000],\n",
       "        [0.1986],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.1395],\n",
       "        [0.1236],\n",
       "        [0.1464],\n",
       "        [0.0489],\n",
       "        [0.1400],\n",
       "        [0.0632],\n",
       "        [0.0469],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0492],\n",
       "        [0.0473],\n",
       "        [0.1448],\n",
       "        [0.1246],\n",
       "        [0.0408],\n",
       "        [0.0570],\n",
       "        [0.0580],\n",
       "        [0.0691],\n",
       "        [0.0392],\n",
       "        [0.0347],\n",
       "        [0.0456],\n",
       "        [0.1448],\n",
       "        [0.0503],\n",
       "        [0.1440],\n",
       "        [0.0534],\n",
       "        [0.0656],\n",
       "        [0.0673],\n",
       "        [0.0312],\n",
       "        [0.0348],\n",
       "        [0.0580],\n",
       "        [0.0486],\n",
       "        [0.1492],\n",
       "        [0.1271],\n",
       "        [0.1492],\n",
       "        [0.0523],\n",
       "        [0.1488],\n",
       "        [0.0676],\n",
       "        [0.0501],\n",
       "        [0.0481],\n",
       "        [0.1438],\n",
       "        [0.1443],\n",
       "        [0.1417],\n",
       "        [0.0490],\n",
       "        [0.0435],\n",
       "        [0.1238],\n",
       "        [0.0606],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0451],\n",
       "        [0.0439],\n",
       "        [0.3292],\n",
       "        [0.0375],\n",
       "        [0.0539],\n",
       "        [0.0528],\n",
       "        [0.0641],\n",
       "        [0.0364],\n",
       "        [0.0316],\n",
       "        [0.0429],\n",
       "        [0.0458],\n",
       "        [0.0539],\n",
       "        [0.0656],\n",
       "        [0.0291],\n",
       "        [0.0297],\n",
       "        [0.0322],\n",
       "        [0.0318],\n",
       "        [0.0328],\n",
       "        [0.0531],\n",
       "        [1.0000],\n",
       "        [0.1376],\n",
       "        [0.0485],\n",
       "        [0.1233],\n",
       "        [0.0665],\n",
       "        [0.0443],\n",
       "        [0.0311],\n",
       "        [0.0396],\n",
       "        [0.1375],\n",
       "        [0.0482],\n",
       "        [0.1396],\n",
       "        [0.1249],\n",
       "        [0.0614],\n",
       "        [0.0300],\n",
       "        [0.0331],\n",
       "        [0.0446],\n",
       "        [0.0462],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.2401],\n",
       "        [0.0386],\n",
       "        [0.3337],\n",
       "        [1.0000],\n",
       "        [1.0000],\n",
       "        [0.0488],\n",
       "        [0.0470],\n",
       "        [0.0406],\n",
       "        [0.0567],\n",
       "        [0.0576],\n",
       "        [0.0686],\n",
       "        [0.0389],\n",
       "        [0.0344],\n",
       "        [0.0453],\n",
       "        [0.0472],\n",
       "        [0.0531],\n",
       "        [0.0687],\n",
       "        [0.0310],\n",
       "        [0.0345],\n",
       "        [0.0577],\n",
       "        [0.0441],\n",
       "        [0.0471],\n",
       "        [0.0664],\n",
       "        [0.0429],\n",
       "        [0.1642],\n",
       "        [0.1582],\n",
       "        [0.0474],\n",
       "        [0.0456],\n",
       "        [0.0479],\n",
       "        [0.3240],\n",
       "        [0.0713],\n",
       "        [0.0373],\n",
       "        [0.0394],\n",
       "        [0.0637],\n",
       "        [0.0550],\n",
       "        [0.0664],\n",
       "        [0.0438],\n",
       "        [0.0767],\n",
       "        [0.0559],\n",
       "        [0.0666],\n",
       "        [0.1998],\n",
       "        [0.0363],\n",
       "        [0.0378],\n",
       "        [0.0310],\n",
       "        [0.0334],\n",
       "        [0.0439],\n",
       "        [0.0391],\n",
       "        [0.0464],\n",
       "        [0.0571],\n",
       "        [0.0516],\n",
       "        [0.0508],\n",
       "        [0.0532],\n",
       "        [0.0614],\n",
       "        [0.0663],\n",
       "        [0.0300],\n",
       "        [0.0330],\n",
       "        [0.0335],\n",
       "        [0.0560],\n",
       "        [0.0443],\n",
       "        [0.0474],\n",
       "        [0.0456],\n",
       "        [0.0476],\n",
       "        [0.0712],\n",
       "        [0.0372],\n",
       "        [0.0394],\n",
       "        [0.0633],\n",
       "        [0.0550],\n",
       "        [0.0663],\n",
       "        [0.0695],\n",
       "        [0.0435],\n",
       "        [0.0438],\n",
       "        [0.0766],\n",
       "        [0.0560],\n",
       "        [0.0666],\n",
       "        [0.0363],\n",
       "        [0.0378],\n",
       "        [0.0310],\n",
       "        [0.0334],\n",
       "        [0.0440],\n",
       "        [0.0389],\n",
       "        [0.0464],\n",
       "        [0.0516],\n",
       "        [0.0613],\n",
       "        [0.0663],\n",
       "        [0.0299],\n",
       "        [0.0301],\n",
       "        [0.0342],\n",
       "        [0.0560],\n",
       "        [0.0442],\n",
       "        [0.0452],\n",
       "        [0.0440],\n",
       "        [0.0376],\n",
       "        [0.0540],\n",
       "        [0.0533],\n",
       "        [0.0642],\n",
       "        [0.1909],\n",
       "        [0.0365],\n",
       "        [0.0319],\n",
       "        [0.0430],\n",
       "        [0.0410],\n",
       "        [0.0459],\n",
       "        [0.0511],\n",
       "        [0.0657],\n",
       "        [0.0297],\n",
       "        [0.0328],\n",
       "        [0.0532],\n",
       "        [0.0462],\n",
       "        [0.1367],\n",
       "        [0.0462],\n",
       "        [0.1229],\n",
       "        [0.1242],\n",
       "        [0.0705],\n",
       "        [0.0369],\n",
       "        [0.0605],\n",
       "        [0.0657],\n",
       "        [0.0425],\n",
       "        [0.0760],\n",
       "        [0.0360],\n",
       "        [0.0307],\n",
       "        [0.0383],\n",
       "        [0.1353],\n",
       "        [0.0475],\n",
       "        [0.1392],\n",
       "        [0.0608],\n",
       "        [0.0616],\n",
       "        [0.0296],\n",
       "        [0.0327],\n",
       "        [0.0522],\n",
       "        [0.0472]], device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att2[1].size(), edge_index.size()\n",
    "att2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class BatchRGAT(torch.nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nrel, nclass, dropout, alpha):\n",
    "        super(BatchRGAT, self).__init__()\n",
    "        torch.manual_seed(999)\n",
    "        self.conv1 = RGATConv(\n",
    "            nfeat, nhid, nrel, dropout=dropout, alpha=alpha, concat=True\n",
    "        )\n",
    "        self.conv2 = RGATConv(\n",
    "            nhid, nclass, nrel, dropout=dropout, alpha=alpha, concat=False\n",
    "        )\n",
    "        self.dropout_ratio = dropout\n",
    "        self.out_classify = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x, out_1, edge_index, edge_type, batch):\n",
    "        x = self.conv1(x, edge_index, edge_type)\n",
    "        x = F.dropout(x, self.dropout_ratio)\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        x = F.elu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return self.out_classify(x + out_1)\n",
    "\n",
    "\n",
    "batch_rgat = BatchRGAT(nfeat, nhid, nrel, nclass, dropout=dropout_ratio, alpha=alpha).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "middle_loader = DataLoader([\n",
    "    Data(x=x[i], edge_index=edge_index, edge_type=edge_type) \n",
    "    for i in range(batch_size)\n",
    "], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([688, 64])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for d in tqdm(middle_loader):\n",
    "#     g_out = batch_rgat(d.x, out_1, d.edge_index, d.edge_type, d.batch)\n",
    "d.x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, trainloader, valloader, edge_index, edge_type):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=args.lr, weight_decay=args.weight_decay\n",
    "    )\n",
    "    loss_fn = nn.BCELoss().cuda()\n",
    "    for epoch in tqdm(range(args.epochs), desc=\"Epochs\"):\n",
    "        loss_train = 0.0\n",
    "        metrics = {k: [] for k in [\"accuracy\", \"roc_auc\", \"f1\", \"recall\", \"precision\"]}\n",
    "        for i, data in enumerate(tqdm(trainloader, desc=\"Training\")):\n",
    "            text_tensor, price_tensor, label_tensor = data\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(text_tensor, price_tensor, edge_index, edge_type)\n",
    "            _output, _label = (\n",
    "                output.argmax(axis=1).unsqueeze(1).float(),\n",
    "                label_tensor.squeeze(0).float(),\n",
    "            )\n",
    "            _loss_train = Variable(loss_fn(_output, _label), requires_grad=True)\n",
    "            metrics[\"accuracy\"].append(accuracy_score(_output.cpu(), _label.cpu()))\n",
    "            metrics[\"roc_auc\"].append(roc_auc_score(_output.cpu(), _label.cpu()))\n",
    "            metrics[\"f1\"].append(f1_score(_output.cpu(), _label.cpu()))\n",
    "            metrics[\"recall\"].append(recall_score(_output.cpu(), _label.cpu()))\n",
    "            metrics[\"precision\"].append(precision_score(_output.cpu(), _label.cpu()))\n",
    "            # backward\n",
    "            _loss_train.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += _loss_train.mean().item()\n",
    "            torch.cuda.empty_cache()\n",
    "        # 每 epoch 計算分類準確率\n",
    "        # trainset\n",
    "        print(\"[Epoch %s] Train Loss: %.3f \\n\" % (epoch, loss_train))\n",
    "        # _, labelss, metrics = get_predictions(clf, trainloader, compute_acc=True, compute_loss=False)\n",
    "        print(\n",
    "            \"    Train F1: %.3f \\n\" % (np.mean(metrics[\"f1\"])),\n",
    "            \"    Train Recall: %.3f \\n\" % (np.mean(metrics[\"recall\"])),\n",
    "            \"    Train Precision: %.3f \\n\" % (np.mean(metrics[\"precision\"])),\n",
    "            \"    Train ROC AUC: %.3f \\n \" % (np.mean(metrics[\"roc_auc\"])),\n",
    "            \"    Train Accuracy: %.3f \\n \" % (np.mean(metrics[\"accuracy\"])),\n",
    "        )\n",
    "        # valset\n",
    "        _, val_labelss, val_metrics, loss_val = get_predictions(\n",
    "            model, valloader, edge_index, edge_type, validation=True\n",
    "        )\n",
    "        print(\"[Epoch %s] Val Loss: %.3f \\n\" % (epoch, loss_val))\n",
    "        print(\n",
    "            \"    Val F1: %.3f \\n\" % (np.mean(val_metrics[\"f1\"])),\n",
    "            \"    Val Recall: %.3f \\n\" % (np.mean(val_metrics[\"recall\"])),\n",
    "            \"    Val Precision: %.3f \\n\" % (np.mean(val_metrics[\"precision\"])),\n",
    "            \"    Val ROC AUC: %.3f \\n \" % (np.mean(val_metrics[\"roc_auc\"])),\n",
    "            \"    Val Accuracy: %.3f \\n \" % (np.mean(val_metrics[\"accuracy\"])),\n",
    "        )\n",
    "        writer.add_scalars(\n",
    "            \"Training vs. Validation Loss\",\n",
    "            {\"Training\": loss_train, \"Validation\": loss_val},\n",
    "            epoch * len(trainloader) + i,\n",
    "        )\n",
    "        for name in metrics:\n",
    "            writer.add_scalars(\n",
    "                f\"Training vs. Validation {name.upper()}\",\n",
    "                {\n",
    "                    \"Training\": np.mean(metrics[name]),\n",
    "                    \"Validation\": np.mean(val_metrics[name]),\n",
    "                },\n",
    "                epoch * len(trainloader) + i,\n",
    "            )\n",
    "        torch.save(model.state_dict(), \"finetuned/mansf-stocknet.pth\")\n",
    "        writer.flush()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario",
   "language": "python",
   "name": "mario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2adbae7e59890a748928459995625d55c9c8d21c81e69bce01be1610eb675e28"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
